{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd399c4d",
   "metadata": {},
   "source": [
    "# Data Processing and Metadata Enrichment Pipeline\n",
    "\n",
    "This notebook guides you through a step-by-step process for loading, processing, and analyzing a dataset using a combination of custom scripts. The workflow includes loading data, creating metadata, filtering data, and performing fuzzy matching.\n",
    "\n",
    "## Steps:\n",
    "1. Set up environment and import necessary modules.\n",
    "2. Define and check dataset directory.\n",
    "3. Load and cache dataframes.\n",
    "4. Create and display metadata.\n",
    "5. Fetch, compare, and configure data fields.\n",
    "6. Filter and process dataframes.\n",
    "7. Perform fuzzy matching.\n",
    "8. Save processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43befc5",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "We begin by importing the necessary libraries and functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57caef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Now, import the necessary custom functions from the scripts\n",
    "\n",
    "from src.scripts.df_metadata import display_metadata_dfs, create_metadata_dfs, enrich_metadata_df\n",
    "from src.scripts.fetch_data_fields import fetch_and_compare_data_fields\n",
    "from src.scripts.build_data_fields_config import build_data_fields_config\n",
    "\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "os.chdir(os.path.join(os.getcwd(), 'src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767e2eb",
   "metadata": {},
   "source": [
    "## Step 2: Define and Check Dataset Directory\n",
    "\n",
    "Define the dataset directory and ensure it exists. This step is crucial as it sets the working directory for subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d55877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.df_generator import get_dataset_directory, check_directory_exists, load_or_cache_dataframes, show_loaded_dfs\n",
    "# Define the dataset directory\n",
    "notebook_directory =os.getcwd()  # This points to the root where the notebook is\n",
    "dataset_directory = os.path.join(notebook_directory, 'dataset')\n",
    "\n",
    "# Check if the dataset directory exists\n",
    "if not check_directory_exists(dataset_directory):\n",
    "    print(f\"Error: Directory '{dataset_directory}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Dataset directory found: {dataset_directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf42d6",
   "metadata": {},
   "source": [
    "## Step 3: Load and Cache DataFrames\n",
    "\n",
    "Load the data from the dataset directory into pandas DataFrames. The data can be loaded from cache or directly from the source files if the cache is not available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store cached DataFrames\n",
    "CACHE_DIR = os.path.join(notebook_directory, 'data', 'cache') \n",
    "\n",
    "# Optionally, you can define a list of specific files to process\n",
    "specific_files = ['fr.openfoodfacts.org.products.csv']  # Set to None to process all files\n",
    "\n",
    "# Load DataFrames from cache or source files\n",
    "dfs = load_or_cache_dataframes(dataset_directory, CACHE_DIR, file_list=specific_files, separator='\\t')\n",
    "\n",
    "# Check if DataFrames are loaded\n",
    "if not dfs:\n",
    "    print(\"No DataFrames were loaded. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded DataFrames: {list(dfs.keys())}\")\n",
    "    show_loaded_dfs(dfs, df_names=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9756",
   "metadata": {},
   "source": [
    "## Step 4: Create and Display Metadata\n",
    "\n",
    "Generate metadata for the loaded DataFrames and display it to understand the structure and content of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21720f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata DataFrames\n",
    "metadata_dfs = create_metadata_dfs(dfs)\n",
    "\n",
    "# Check if metadata DataFrames were created\n",
    "if not metadata_dfs:\n",
    "    print(\"No metadata DataFrames were created. Exiting.\")\n",
    "else:\n",
    "    print(f\"Created Metadata DataFrames: {list(metadata_dfs.keys())}\")\n",
    "    display_metadata_dfs(metadata_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0892c03",
   "metadata": {},
   "source": [
    "## Step 5: Fetch, Compare, and Configure Data Fields\n",
    "\n",
    "Fetch and compare data fields from the dataset, and build the necessary configuration files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7365c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(notebook_directory,'data')\n",
    "\n",
    "# Run the fetch and compare data fields script\n",
    "fetch_and_compare_data_fields(DATA_DIR)\n",
    "\n",
    "# Build the config file\n",
    "build_data_fields_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78565870",
   "metadata": {},
   "source": [
    "## Step 6: Filter and Process DataFrames\n",
    "\n",
    "Filter the metadata and corresponding DataFrames, and save the filtered data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bae2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config.json\n",
    "script_dir = os.path.join(notebook_directory,'scritps')\n",
    "config_path = os.path.join(notebook_directory, 'config', 'data_fields_config.json')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Enrich the metadata DataFrame\n",
    "combined_metadata = pd.concat(metadata_dfs.values(), keys=metadata_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'DataFrame'})\n",
    "combined_metadata = enrich_metadata_df(combined_metadata, config)\n",
    "\n",
    "\n",
    "# Save the combined metadata DataFrame to a CSV file\n",
    "output_dir = os.path.join(notebook_directory, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "combined_metadata_path = os.path.join(output_dir, 'combined_metadata.csv')\n",
    "combined_metadata.to_csv(combined_metadata_path, index=False)\n",
    "print(f\"Combined metadata {combined_metadata.shape} has been saved or updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fd368",
   "metadata": {},
   "source": [
    "## Step 7: Identify columns cluster\n",
    "\n",
    "Checking cluster of columns based on Duplicate(%) and Fill(%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e66d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_metadata_clusters import run_dash_app\n",
    "\n",
    "# Run the Dash app  \n",
    "run_dash_app(combined_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62065b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.df_filtering import filter_metadata_and_dataframes, process_dataframe\n",
    "from src.scripts.df_fuzzywuzzy import fuzzy_dataframe\n",
    "\n",
    "\n",
    "# Specify your datetime checks as a list of tuples\n",
    "datetime_checks = [\n",
    "    # ('created_t', 'created_datetime'),\n",
    "    # ('last_modified_t', 'last_modified_datetime')\n",
    "]\n",
    "\n",
    "# Specify your field frequency checks as a list of tuples\n",
    "field_checks = [\n",
    "    #(['countries', 'countries_tags', 'countries_fr'], 'countries'),\n",
    "    #(['ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n'], 'ingredients_palm_oil'),\n",
    "    (['nutrition_grade_fr', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g'], 'nutrition'),\n",
    "    #(['brands_tags', 'brands'], 'brands'),\n",
    "    #(['additives_n', 'additives', 'additives_tags', 'additives_fr'], 'additives'),\n",
    "    #(['states', 'states_tags', 'states_fr'], 'states'),\n",
    "    (['pnns_groups_1', 'pnns_groups_2'], 'pnns_groups')\n",
    "    \n",
    "]\n",
    "\n",
    "# Columns to check for at least one non-null value\n",
    "columns_to_check = [\n",
    "    'nutrition_grade_fr', 'energy_100g', 'fat_100g', 'saturated-fat_100g',\n",
    "    'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g',\n",
    "    'fiber_100g', 'proteins_100g', 'salt_100g', 'sodium_100g', 'vitamin-a_100g',\n",
    "    'vitamin-c_100g', 'calcium_100g', 'iron_100g', 'nutrition-score-fr_100g',\n",
    "    'nutrition-score-uk_100g'\n",
    "]\n",
    "\n",
    "# Fields to be deleted after anaylysis\n",
    "fields_to_delete = [\n",
    "    'url', 'created_t', 'created_datetime','last_modified_t', 'last_modified_datetime', 'states', 'states_tags', 'states_fr', 'countries', 'countries_tags', 'countries_fr',\n",
    "    'brands_tags', 'brands', 'additives_n', 'additives', 'additives_tags', 'additives_fr',\n",
    "    'creator','ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n',\n",
    "    'quantity', 'serving_size', 'additives', 'ingredients_text','product_name',\n",
    "    'categories','categories_tags','categories_fr','packaging','packaging_tags','image_url','image_small_url','main_category','main_category_fr'\n",
    "    ]\n",
    "\n",
    "# Now process your specific DataFrame\n",
    "df_name = 'fr.openfoodfacts.org.products'\n",
    "\n",
    "# Filter and process DataFrame in one step\n",
    "if df_name in dfs:\n",
    "    combined_metadata, filtered_dfs = filter_metadata_and_dataframes(combined_metadata, dfs, 20)\n",
    "    process_dataframe(filtered_dfs[df_name], log_dir='logs', temp_dir='temp', datetime_checks=datetime_checks, field_checks=field_checks)\n",
    "    fuzzy_dataframe(temp_dir='temp', config_dir='config', checks=field_checks, threshold=90)\n",
    "\n",
    "    # Drop rows where all specified columns are null\n",
    "    filtered_dfs[df_name].dropna(subset=columns_to_check, how='all', inplace=True)\n",
    "\n",
    "    # Drop duplicates after filtering rows\n",
    "    filtered_dfs[df_name].drop_duplicates(inplace=True)\n",
    "\n",
    "    # Delete the specified columns from combined_metadata and update the related DataFrame\n",
    "    combined_metadata = combined_metadata[~combined_metadata['Column Name'].isin(fields_to_delete)]\n",
    "    filtered_dfs[df_name] = filtered_dfs[df_name][combined_metadata['Column Name']]\n",
    "\n",
    "    # Save the processed DataFrame to the dataset directory\n",
    "    dataset_path = os.path.join('dataset', f'processed_{df_name}.csv')\n",
    "    filtered_dfs[df_name].to_csv(dataset_path, index=False)\n",
    "    \n",
    "    # Save the updated metadata to the data directory\n",
    "    metadata_path = os.path.join('data', f'processed_metadata.csv')\n",
    "    combined_metadata.to_csv(metadata_path, index=False)\n",
    "    \n",
    "    print(f\"Processed DataFrame '{df_name}' and metadata have been saved.\")\n",
    "else:\n",
    "    print(f\"DataFrame '{df_name}' not found in the loaded DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_nutriscore import run_dash_app_nutriscore, safe_eval\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "#os.chdir(os.path.join(os.getcwd(), 'src'))\n",
    "#notebook_directory =os.getcwd()\n",
    "\n",
    "# Assuming notebook_directory is already defined in your notebook\n",
    "nutriscore_directory = os.path.join(notebook_directory, 'temp', 'nutrition_combination_log.csv')\n",
    "nutriscore = pd.read_csv(nutriscore_directory)\n",
    "\n",
    "\n",
    "\n",
    "run_dash_app_nutriscore(nutriscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87eb0f",
   "metadata": {},
   "source": [
    "### Nutrient Maximum Limits Justification\n",
    "\n",
    "The following maximum limits are established to ensure that the values in each column of your dataset remain within a realistic and scientifically accurate range. These limits are based on general nutritional guidelines, food composition databases, and what is considered physiologically plausible for the nutrients and metrics in question.\n",
    "\n",
    "#### 1. **Energy (energy_100g): 900 kcal/100g**\n",
    "- **Justification**: The upper limit of 900 kcal per 100 grams is based on the highest energy-dense foods, such as pure fats and oils. For instance, oils like olive oil can contain up to 884 kcal per 100g, making 900 kcal a reasonable upper boundary to catch extreme outliers that could suggest errors in data entry.\n",
    "\n",
    "#### 2. **Fat (fat_100g): 100g/100g**\n",
    "- **Justification**: Pure fat, like oils and lard, contains 100g of fat per 100g. This limit reflects the fact that no food item should logically contain more fat than its total weight, so 100g/100g is the natural upper boundary.\n",
    "\n",
    "#### 3. **Saturated Fat (saturated-fat_100g): 50g/100g**\n",
    "- **Justification**: Saturated fat typically makes up a portion of total fat content. For high-saturated fat products like butter, which may have up to 50-60% saturated fat, a limit of 50g/100g ensures that products are within expected ranges.\n",
    "\n",
    "#### 4. **Carbohydrates (carbohydrates_100g): 100g/100g**\n",
    "- **Justification**: Similar to fat, carbohydrates can theoretically make up 100% of a food's weight. However, this is rare, as most foods contain a mix of nutrients. This limit helps identify potential errors where carbohydrate content might have been overstated.\n",
    "\n",
    "#### 5. **Sugars (sugars_100g): 100g/100g**\n",
    "- **Justification**: Sugars, a subset of carbohydrates, can also theoretically reach 100g/100g in foods composed entirely of sugar (e.g., pure glucose or sucrose). The 100g/100g limit ensures that entries exceeding this amount are flagged as potential errors.\n",
    "\n",
    "#### 6. **Sodium (sodium_100g): 2.3g/100g**\n",
    "- **Justification**: The Dietary Guidelines for Americans recommend a maximum sodium intake of 2,300 mg per day. While this applies to daily intake, 2.3g per 100g in foods represents a high sodium concentration typical in heavily salted products. Foods like soy sauce can reach these levels, but it remains an upper threshold to catch extreme cases.\n",
    "\n",
    "#### 7. **Salt (salt_100g): 5.75g/100g**\n",
    "- **Justification**: Salt is composed of sodium (40%) and chloride (60%). If sodium is at its upper limit of 2.3g/100g, the corresponding salt content would be around 5.75g/100g. This ensures that the sodium-salt relationship is maintained within logical bounds.\n",
    "\n",
    "#### 8. **Trans Fat (trans-fat_100g): 55.33g/100g**\n",
    "- **Justification**: Trans fats are typically found in processed foods, and while 55.33g/100g is higher than what would be found in natural foods, this limit allows for capturing high-trans fat industrial products, though it is still within a physiologically plausible range.\n",
    "\n",
    "#### 9. **Cholesterol (cholesterol_100g): 55.08mg/100g**\n",
    "- **Justification**: Cholesterol content in food can vary widely, with organ meats like liver containing very high levels. A limit of 55.08mg/100g provides a boundary that accommodates high-cholesterol foods without allowing for implausible entries.\n",
    "\n",
    "#### 10. **Fiber (fiber_100g): 99.49g/100g**\n",
    "- **Justification**: Fiber is a non-digestible carbohydrate. Foods high in fiber, like bran, can contain significant amounts, though a fiber content near 100% would be extremely rare. The limit ensures the integrity of fiber content without exceeding realistic values.\n",
    "\n",
    "#### 11. **Proteins (proteins_100g): 99.04g/100g**\n",
    "- **Justification**: Protein content can be very high in certain food products, like protein supplements. The upper limit of 99.04g/100g ensures that entries are realistic, as 100% protein content would be nearly impossible in natural foods.\n",
    "\n",
    "#### 12. **Vitamin A (vitamin-a_100g): 57.12mg/100g**\n",
    "- **Justification**: Vitamin A levels can be very high in foods like liver, but 57.12mg/100g represents an upper limit where concentrations beyond this could indicate a data entry error, as such high levels are unusual.\n",
    "\n",
    "#### 13. **Vitamin C (vitamin-c_100g): 56.09mg/100g**\n",
    "- **Justification**: Foods rich in Vitamin C, such as certain fruits, can have high concentrations, but levels above this are uncommon. This limit helps to filter out improbable values.\n",
    "\n",
    "#### 14. **Calcium (calcium_100g): 56.03mg/100g**\n",
    "- **Justification**: High-calcium foods like dairy products can approach these levels, but any values above this would likely be due to fortification or data errors. The limit ensures consistency with expected nutritional values.\n",
    "\n",
    "#### 15. **Iron (iron_100g): 56.21mg/100g**\n",
    "- **Justification**: Iron-rich foods like red meat and fortified cereals can have significant iron content. However, 56.21mg/100g acts as a reasonable upper bound, capturing outliers without excluding naturally iron-rich foods.\n",
    "\n",
    "### Additional Justification:\n",
    "- **Nutritional Guidelines**: These limits are set based on standard nutritional data and guidelines provided by sources such as the USDA Food Composition Databases, European Food Safety Authority (EFSA), and general dietary recommendations.\n",
    "- **Data Integrity**: These limits also ensure that the data is free from common errors, such as mistyping or incorrect unit conversions, which could lead to implausible nutrient values.\n",
    "\n",
    "By setting these limits, the script can effectively identify and remove outliers or erroneous entries, ensuring that the dataset is clean and reliable for further analysis or reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1bcd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.df_generator import check_directory_exists, load_or_cache_dataframes, show_loaded_dfs\n",
    "from src.scripts.df_business_data_integrity import run_integrity_check\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "os.chdir(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Define the dataset directory\n",
    "notebook_directory = os.getcwd()  # This points to the root where the notebook is\n",
    "dataset_directory = os.path.join(notebook_directory, 'dataset')\n",
    "\n",
    "# Check if the dataset directory exists\n",
    "if not check_directory_exists(dataset_directory):\n",
    "    print(f\"Error: Directory '{dataset_directory}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Dataset directory found: {dataset_directory}\")\n",
    "\n",
    "# Directory to store cached DataFrames\n",
    "CACHE_DIR = os.path.join(notebook_directory, 'data', 'cache') \n",
    "\n",
    "# Optionally, you can define a list of specific files to process\n",
    "specific_files = ['processed_fr.openfoodfacts.org.products.csv']\n",
    "\n",
    "# Load DataFrames from cache or source files\n",
    "processed_dfs = load_or_cache_dataframes(dataset_directory, CACHE_DIR, file_list=specific_files, separator=',')\n",
    "\n",
    "# Check if DataFrames are loaded\n",
    "if not processed_dfs:\n",
    "    print(\"No DataFrames were loaded. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded DataFrames: {list(processed_dfs.keys())}\")\n",
    "    show_loaded_dfs(processed_dfs, df_names=None)\n",
    "\n",
    "# Get the specific DataFrame for processing\n",
    "df_name = 'processed_fr.openfoodfacts.org.products'\n",
    "if df_name in processed_dfs:\n",
    "    df = processed_dfs[df_name]  # Pass the DataFrame directly if not a dictionary\n",
    "    run_integrity_check(df, log_dir='logs')\n",
    "else:\n",
    "    print(f\"DataFrame '{df_name}' not found in the loaded DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
