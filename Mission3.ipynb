{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd399c4d",
   "metadata": {},
   "source": [
    "# Data Processing and Metadata Enrichment Pipeline\n",
    "\n",
    "This notebook guides you through a step-by-step process for loading, processing, and analyzing a dataset using a combination of custom scripts. The workflow includes loading data, creating metadata, filtering data, and performing fuzzy matching.\n",
    "\n",
    "## Steps:\n",
    "1. Set up environment and import necessary modules.\n",
    "2. Define and check dataset directory.\n",
    "3. Load and cache dataframes.\n",
    "4. Create and display metadata.\n",
    "5. Fetch, compare, and configure data fields.\n",
    "6. Filter and process dataframes.\n",
    "7. Perform fuzzy matching.\n",
    "8. Save processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43befc5",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "We begin by importing the necessary libraries and functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57caef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Now, import the necessary custom functions from the scripts\n",
    "\n",
    "from src.scripts.df_metadata import display_metadata_dfs, create_metadata_dfs, enrich_metadata_df\n",
    "from src.scripts.fetch_data_fields import fetch_and_compare_data_fields\n",
    "from src.scripts.build_data_fields_config import build_data_fields_config\n",
    "\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "os.chdir(os.path.join(os.getcwd(), 'src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767e2eb",
   "metadata": {},
   "source": [
    "## Step 2: Define and Check Dataset Directory\n",
    "\n",
    "Define the dataset directory and ensure it exists. This step is crucial as it sets the working directory for subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d55877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory found\n"
     ]
    }
   ],
   "source": [
    "from src.scripts.df_generator import check_directory_exists, load_or_cache_dataframes, show_loaded_dfs\n",
    "# Define the dataset directory\n",
    "notebook_directory =os.getcwd()  # This points to the root where the notebook is\n",
    "dataset_directory = os.path.join(notebook_directory, 'dataset')\n",
    "\n",
    "# Check if the dataset directory exists\n",
    "if not check_directory_exists(dataset_directory):\n",
    "    print(f\"Error: Directory dataset does not exist.\")\n",
    "else:\n",
    "    print(f\"Dataset directory found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf42d6",
   "metadata": {},
   "source": [
    "## Step 3: Load and Cache DataFrames\n",
    "\n",
    "Load the data from the dataset directory into pandas DataFrames. The data can be loaded from cache or directly from the source files if the cache is not available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f4f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'fr.openfoodfacts.org.products' from cache.\n",
      "Nullity matrix for 'fr.openfoodfacts.org.products' has been generated.\n",
      "Loaded DataFrames: ['fr.openfoodfacts.org.products']\n",
      "Currently loaded DataFrames:\n",
      "DataFrame for file 'fr.openfoodfacts.org.products (320767, 146)':\n",
      "            code                                                url  ... nutrition-score-fr_100g nutrition-score-uk_100g\n",
      "0  0000000003087  http://world-fr.openfoodfacts.org/produit/0000...  ...                     NaN                     NaN\n",
      "1  0000000004530  http://world-fr.openfoodfacts.org/produit/0000...  ...                    14.0                    14.0\n",
      "2  0000000004559  http://world-fr.openfoodfacts.org/produit/0000...  ...                     0.0                     0.0\n",
      "3  0000000016087  http://world-fr.openfoodfacts.org/produit/0000...  ...                    12.0                    12.0\n",
      "4  0000000016094  http://world-fr.openfoodfacts.org/produit/0000...  ...                     NaN                     NaN\n",
      "\n",
      "[5 rows x 146 columns]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Directory to store cached DataFrames\n",
    "CACHE_DIR = os.path.join(notebook_directory, 'data', 'cache') \n",
    "\n",
    "# Optionally, you can define a list of specific files to process\n",
    "specific_files = ['fr.openfoodfacts.org.products.csv']  # Set to None to process all files\n",
    "\n",
    "# Load DataFrames from cache or source files\n",
    "dfs = load_or_cache_dataframes(dataset_directory, CACHE_DIR, file_list=specific_files, separator='\\t')\n",
    "\n",
    "# Check if DataFrames are loaded\n",
    "if not dfs:\n",
    "    print(\"No DataFrames were loaded. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded DataFrames: {list(dfs.keys())}\")\n",
    "    show_loaded_dfs(dfs, df_names=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9756",
   "metadata": {},
   "source": [
    "## Step 4: Create and Display Metadata\n",
    "\n",
    "Generate metadata for the loaded DataFrames and display it to understand the structure and content of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21720f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Metadata DataFrames: ['fr.openfoodfacts.org.products']\n",
      "Metadata for fr.openfoodfacts.org.products (146, 8):\n",
      "                          Column Name    Dtype  ... Duplicate Percentage  Missing Percentage\n",
      "0                                code   object  ...             0.000000            0.007170\n",
      "1                                 url   object  ...             0.000000            0.007170\n",
      "2                             creator   object  ...            98.897947            0.000624\n",
      "3                           created_t   object  ...            40.901410            0.000935\n",
      "4                    created_datetime   object  ...            40.899993            0.002806\n",
      "..                                ...      ...  ...                  ...                 ...\n",
      "141  collagen-meat-protein-ratio_100g  float64  ...            96.363636           99.948561\n",
      "142                        cocoa_100g  float64  ...            91.139241           99.704458\n",
      "143             carbon-footprint_100g  float64  ...            24.626866           99.916450\n",
      "144           nutrition-score-fr_100g  float64  ...            99.975136           31.038417\n",
      "145           nutrition-score-uk_100g  float64  ...            99.975136           31.038417\n",
      "\n",
      "[146 rows x 8 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create metadata DataFrames\n",
    "metadata_dfs = create_metadata_dfs(dfs)\n",
    "\n",
    "# Check if metadata DataFrames were created\n",
    "if not metadata_dfs:\n",
    "    print(\"No metadata DataFrames were created. Exiting.\")\n",
    "else:\n",
    "    print(f\"Created Metadata DataFrames: {list(metadata_dfs.keys())}\")\n",
    "    display_metadata_dfs(metadata_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0892c03",
   "metadata": {},
   "source": [
    "## Step 5: Fetch, Compare, and Configure Data Fields\n",
    "\n",
    "Fetch and compare data fields from the dataset, and build the necessary configuration files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7365c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HISTORY_DIR: True\n",
      "Created DIFF_DIR: True\n",
      "No changes detected on data\\data_fields.txt.\n",
      "Config file 'data_fields_config.json' has been updated and saved.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(notebook_directory,'data')\n",
    "\n",
    "# Run the fetch and compare data fields script\n",
    "fetch_and_compare_data_fields(DATA_DIR)\n",
    "\n",
    "# Build the config file\n",
    "build_data_fields_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78565870",
   "metadata": {},
   "source": [
    "## Step 6: Filter and Process DataFrames\n",
    "\n",
    "Filter the metadata and corresponding DataFrames, and save the filtered data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8bae2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined metadata (146, 11) has been saved or updated.\n"
     ]
    }
   ],
   "source": [
    "# Load the config.json\n",
    "script_dir = os.path.join(notebook_directory,'scritps')\n",
    "config_path = os.path.join(notebook_directory, 'config', 'data_fields_config.json')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Enrich the metadata DataFrame\n",
    "combined_metadata = pd.concat(metadata_dfs.values(), keys=metadata_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'DataFrame'})\n",
    "combined_metadata = enrich_metadata_df(combined_metadata, config)\n",
    "\n",
    "\n",
    "# Save the combined metadata DataFrame to a CSV file\n",
    "output_dir = os.path.join(notebook_directory, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "combined_metadata_path = os.path.join(output_dir, 'combined_metadata.csv')\n",
    "combined_metadata.to_csv(combined_metadata_path, index=False)\n",
    "print(f\"Combined metadata {combined_metadata.shape} has been saved or updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fd368",
   "metadata": {},
   "source": [
    "## Step 7: Identify columns cluster\n",
    "\n",
    "Checking cluster of columns based on Duplicate(%) and Fill(%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e66d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_metadata_clusters import run_dash_app\n",
    "\n",
    "combined_metadata_cluster = combined_metadata.copy()\n",
    "\n",
    "# Run the Dash app  \n",
    "run_dash_app(combined_metadata_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e40ee",
   "metadata": {},
   "source": [
    "## Step 8: Analyse, fuzzy and other stuff\n",
    "\n",
    "blabla\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c62065b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f042de21b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame 'fr.openfoodfacts.org.products' to retain only relevant columns.\n",
      "Check the nutrition combination file for more details about fields frequency.\n",
      "Check the pnns_groups combination file for more details about fields frequency.\n",
      "[nutrition] Total combinations to process: 441\n",
      "[nutrition] Finished processing all combinations.\n",
      "Grouped results for nutrition have been generated\n",
      "[pnns_groups] Total combinations to process: 44\n",
      "[pnns_groups] Finished processing all combinations.\n",
      "Grouped results for pnns_groups have been generated\n",
      "Fuzzy matching and grouping completed for all checks.\n",
      "Processed DataFrame 'fr.openfoodfacts.org.products' and metadata have been saved.\n"
     ]
    }
   ],
   "source": [
    "from src.scripts.df_filtering import filter_metadata_and_dataframes, process_dataframe\n",
    "from src.scripts.df_fuzzywuzzy import fuzzy_dataframe\n",
    "\n",
    "\n",
    "# Specify your datetime checks as a list of tuples\n",
    "datetime_checks = [\n",
    "    # ('created_t', 'created_datetime'),\n",
    "    # ('last_modified_t', 'last_modified_datetime')\n",
    "]\n",
    "\n",
    "# Specify your field frequency checks as a list of tuples\n",
    "field_checks = [\n",
    "    #(['countries', 'countries_tags', 'countries_fr'], 'countries'),\n",
    "    #(['ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n'], 'ingredients_palm_oil'),\n",
    "    (['nutrition_grade_fr', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g'], 'nutrition'),\n",
    "    #(['brands_tags', 'brands'], 'brands'),\n",
    "    #(['additives_n', 'additives', 'additives_tags', 'additives_fr'], 'additives'),\n",
    "    #(['states', 'states_tags', 'states_fr'], 'states'),\n",
    "    (['pnns_groups_1', 'pnns_groups_2'], 'pnns_groups')\n",
    "    \n",
    "]\n",
    "\n",
    "# Columns to check for at least one non-null value\n",
    "columns_to_check = [\n",
    "    'nutrition_grade_fr', 'energy_100g', 'fat_100g', 'saturated-fat_100g',\n",
    "    'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g',\n",
    "    'fiber_100g', 'proteins_100g', 'salt_100g', 'sodium_100g', 'vitamin-a_100g',\n",
    "    'vitamin-c_100g', 'calcium_100g', 'iron_100g', 'nutrition-score-fr_100g',\n",
    "    'nutrition-score-uk_100g'\n",
    "]\n",
    "\n",
    "# Fields to be deleted after anaylysis\n",
    "fields_to_delete = [\n",
    "    'url', 'created_t', 'created_datetime','last_modified_t', 'last_modified_datetime', 'states', 'states_tags', 'states_fr', 'countries', 'countries_tags', 'countries_fr',\n",
    "    'brands_tags', 'brands', 'additives_n', 'additives', 'additives_tags', 'additives_fr',\n",
    "    'creator','ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n',\n",
    "    'quantity', 'serving_size', 'additives', 'ingredients_text','product_name',\n",
    "    'categories','categories_tags','categories_fr','packaging','packaging_tags','image_url','image_small_url','main_category','main_category_fr'\n",
    "    ]\n",
    "\n",
    "# Now process your specific DataFrame\n",
    "df_name = 'fr.openfoodfacts.org.products'\n",
    "\n",
    "# Filter and process DataFrame in one step\n",
    "if df_name in dfs:\n",
    "    combined_metadata, filtered_dfs = filter_metadata_and_dataframes(combined_metadata, dfs, 20)\n",
    "    process_dataframe(filtered_dfs[df_name], log_dir='logs', temp_dir='temp', datetime_checks=datetime_checks, field_checks=field_checks)\n",
    "    fuzzy_dataframe(temp_dir='temp', config_dir='config', checks=field_checks, threshold=90)\n",
    "\n",
    "    # Drop rows where all specified columns are null\n",
    "    filtered_dfs[df_name].dropna(subset=columns_to_check, how='all', inplace=True)\n",
    "\n",
    "    # Drop duplicates after filtering rows\n",
    "    filtered_dfs[df_name].drop_duplicates(inplace=True)\n",
    "\n",
    "    # Delete the specified columns from combined_metadata and update the related DataFrame\n",
    "    combined_metadata = combined_metadata[~combined_metadata['Column Name'].isin(fields_to_delete)]\n",
    "    filtered_dfs[df_name] = filtered_dfs[df_name][combined_metadata['Column Name']]\n",
    "\n",
    "    # Save the processed DataFrame to the dataset directory\n",
    "    dataset_path = os.path.join('dataset', f'processed_{df_name}.csv')\n",
    "    filtered_dfs[df_name].to_csv(dataset_path, index=False)\n",
    "    \n",
    "    # Save the updated metadata to the data directory\n",
    "    metadata_path = os.path.join('data', f'processed_metadata.csv')\n",
    "    combined_metadata.to_csv(metadata_path, index=False)\n",
    "    \n",
    "    print(f\"Processed DataFrame '{df_name}' and metadata have been saved.\")\n",
    "else:\n",
    "    print(f\"DataFrame '{df_name}' not found in the loaded DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2df568",
   "metadata": {},
   "source": [
    "## Step 9: Nutrition score Clustering Dashboard\n",
    "\n",
    "blabla\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ea5846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8051/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f0440fd7c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.scripts.plot_nutriscore import run_dash_app_nutriscore, safe_eval\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "#os.chdir(os.path.join(os.getcwd(), 'src'))\n",
    "#notebook_directory =os.getcwd()\n",
    "\n",
    "# Assuming notebook_directory is already defined in your notebook\n",
    "nutriscore_directory = os.path.join(notebook_directory, 'temp', 'nutrition_combination_log.csv')\n",
    "nutriscore = pd.read_csv(nutriscore_directory)\n",
    "\n",
    "\n",
    "\n",
    "run_dash_app_nutriscore(nutriscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87eb0f",
   "metadata": {},
   "source": [
    "### Nutrient Maximum Limits Justification\n",
    "\n",
    "| Nutrient                      | Maximum Limit      | Justification |\n",
    "|-------------------------------|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Energy (energy_100g)**       | 950 kcal/100g      | The upper limit of 950 kcal per 100g accounts for extremely energy-dense foods like pure oils and concentrated products, while capturing potential data entry errors without excluding valid outliers.                                                                                                       |\n",
    "| **Fat (fat_100g)**             | 95g/100g           | While pure fat can reach 100g/100g, lowering the limit slightly to 95g/100g flags potential rounding errors in data entry, as it's rare for foods to contain exactly 100g of fat.                                                                                                                             |\n",
    "| **Saturated Fat (saturated-fat_100g)** | 55g/100g  | High-saturated fat products like butter can have up to 50-60% saturated fat. A limit of 55g/100g allows flexibility for processed fats while still flagging extreme cases.                                                                                                                                   |\n",
    "| **Carbohydrates (carbohydrates_100g)** | 95g/100g | Carbohydrates can theoretically reach 100% of a food's weight, but setting the limit at 95g/100g helps to flag data entry errors while accommodating foods with high carbohydrate content.                                                                                                                  |\n",
    "| **Sugars (sugars_100g)**       | 95g/100g           | Sugars, although able to reach 100g/100g, are rarely that high in practice. Setting the limit at 95g/100g captures realistic values while identifying potential overstatements.                                                                                                                               |\n",
    "| **Sodium (sodium_100g)**       | 3g/100g            | While most foods don't exceed 2.3g/100g, certain salt-heavy products like salted meats or fish can reach higher sodium levels. A 3g/100g limit captures these outliers while maintaining realistic boundaries.                                                                                                 |\n",
    "| **Salt (salt_100g)**           | 6g/100g            | With sodium reaching 3g/100g in some extreme cases, the corresponding salt content would be around 6g/100g, maintaining logical sodium-salt relationships for highly salted products.                                                                                                                         |\n",
    "| **Trans Fat (trans-fat_100g)**  | 5g/100g            | Modern food regulations limit trans fats in many countries, making it rare for foods to exceed 5g/100g. This lower limit ensures compliance with current guidelines and excludes unrealistic trans fat levels.                                                                                                |\n",
    "| **Cholesterol (cholesterol_100g)** | 500mg/100g     | High-cholesterol foods like organ meats are accommodated, but a higher limit of 500mg/100g better captures naturally high-cholesterol foods without excluding legitimate entries.                                                                                                                             |\n",
    "| **Fiber (fiber_100g)**         | 50g/100g           | Fiber content can be high in foods like bran, but a limit of 50g/100g ensures that even fiber-dense products are realistically capped, filtering out unrealistic entries.                                                                                                                                     |\n",
    "| **Proteins (proteins_100g)**   | 90g/100g           | High-protein products, especially supplements, can reach up to 90g/100g. This limit allows for protein-dense foods while filtering out implausible data entries.                                                                                                                                              |\n",
    "| **Vitamin A (vitamin-a_100g)** | 30mg/100g          | Foods like liver can contain high levels of Vitamin A, but 30mg/100g is a more conservative upper limit to ensure that extreme, potentially toxic levels are flagged as data errors.                                                                                                                           |\n",
    "| **Vitamin C (vitamin-c_100g)** | 50mg/100g          | While some fruits have high Vitamin C concentrations, a 50mg/100g limit is sufficient to capture natural sources while identifying improbable values.                                                                                                                                                         |\n",
    "| **Calcium (calcium_100g)**     | 30mg/100g          | Although fortified foods may exceed natural calcium levels, 30mg/100g is a reasonable limit that captures high-calcium foods while excluding artificially inflated entries.                                                                                                                                     |\n",
    "| **Iron (iron_100g)**           | 40mg/100g          | Iron-rich foods like red meat and fortified cereals are accommodated, but a 40mg/100g limit is more realistic for naturally occurring iron levels, preventing data entry errors.                                                                                                                               |\n",
    "\n",
    "### Additional Justification:\n",
    "- **Nutritional Guidelines**: Limits are based on standard nutritional data from sources such as USDA, EFSA, and general dietary recommendations.\n",
    "- **Data Integrity**: These limits ensure data is free from common errors (e.g., mistyping, incorrect unit conversions), helping to maintain clean, reliable data for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f1bcd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory found\n",
      "Loaded 'processed_fr.openfoodfacts.org.products' from cache.\n",
      "Nullity matrix for 'processed_fr.openfoodfacts.org.products' has been generated.\n",
      "Loaded DataFrames: ['processed_fr.openfoodfacts.org.products']\n",
      "Currently loaded DataFrames:\n",
      "DataFrame for file 'processed_fr.openfoodfacts.org.products (262828, 21)':\n",
      "            code nutrition_grade_fr  ... nutrition-score-fr_100g nutrition-score-uk_100g\n",
      "0  0000000004530                  d  ...                    14.0                    14.0\n",
      "1  0000000004559                  b  ...                     0.0                     0.0\n",
      "2  0000000016087                  d  ...                    12.0                    12.0\n",
      "3  0000000016094                NaN  ...                     NaN                     NaN\n",
      "4  0000000016100                NaN  ...                     NaN                     NaN\n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "\n",
      "pnns_groups_1 and pnns_groups_2 combinations have been standardized.\n",
      "Processed DataFrame 'processed_fr.openfoodfacts.org.products' has been updated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.scripts.df_generator import check_directory_exists, load_or_cache_dataframes, show_loaded_dfs\n",
    "from src.scripts.df_business_data_integrity import run_integrity_check\n",
    "from src.scripts.df_pnns_group import standardize_pnns_groups\n",
    "from src.scripts.df_nutriscore import check_and_standardize_nutrition_grades\n",
    "\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "\n",
    "# Define the dataset directory\n",
    "notebook_directory = os.getcwd()  # This points to the root where the notebook is\n",
    "dataset_directory = os.path.join(notebook_directory, 'dataset')\n",
    "json_file_path = os.path.join(notebook_directory, 'config', 'pnns_groups_grouped_results.json')\n",
    "\n",
    "# Check if the dataset directory exists\n",
    "if not check_directory_exists(dataset_directory):\n",
    "    print(f\"Error: Directory dataset does not exist.\")\n",
    "else:\n",
    "    print(f\"Dataset directory found\")\n",
    "\n",
    "# Directory to store cached DataFrames\n",
    "CACHE_DIR = os.path.join(notebook_directory, 'data', 'cache')\n",
    "\n",
    "# Optionally, you can define a list of specific files to process\n",
    "specific_files = ['processed_fr.openfoodfacts.org.products.csv']\n",
    "\n",
    "# Load DataFrames from cache or source files\n",
    "processed_dfs = load_or_cache_dataframes(dataset_directory, CACHE_DIR, file_list=specific_files, separator=',')\n",
    "\n",
    "# Check if DataFrames are loaded\n",
    "if not processed_dfs:\n",
    "    print(\"No DataFrames were loaded. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded DataFrames: {list(processed_dfs.keys())}\")\n",
    "    show_loaded_dfs(processed_dfs, df_names=None)\n",
    "\n",
    "# Get the specific DataFrame for processing\n",
    "df_name = 'processed_fr.openfoodfacts.org.products'\n",
    "if df_name in processed_dfs:\n",
    "    df = processed_dfs[df_name]  # Pass the DataFrame directly if not a dictionary\n",
    "    run_integrity_check(df, log_dir='logs')\n",
    "    standardize_pnns_groups(json_file_path) \n",
    "    check_and_standardize_nutrition_grades(df)\n",
    "    \n",
    "    # Save the processed DataFrame to the dataset directory\n",
    "    dataset_path = os.path.join('dataset', f'processed_{df_name}.csv')\n",
    "    processed_dfs[df_name].to_csv(dataset_path, index=False)\n",
    "    print(f\"Processed DataFrame '{df_name}' has been updated.\")\n",
    "else:\n",
    "    print(f\"DataFrame '{df_name}' not found in the loaded DataFrames.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
