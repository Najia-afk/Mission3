{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd399c4d",
   "metadata": {},
   "source": [
    "# Data Processing and Metadata Enrichment Pipeline\n",
    "\n",
    "This notebook guides you through a step-by-step process for loading, processing, and analyzing a dataset using a combination of custom scripts. The workflow includes loading data, creating metadata, filtering data, and performing fuzzy matching.\n",
    "\n",
    "## Steps:\n",
    "1. Set up environment and import necessary modules.\n",
    "2. Define and check dataset directory.\n",
    "3. Load and cache dataframes.\n",
    "4. Create and display metadata.\n",
    "5. Fetch, compare, and configure data fields.\n",
    "6. Filter and process dataframes.\n",
    "7. Perform fuzzy matching.\n",
    "8. Save processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43befc5",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "We begin by importing the necessary libraries and functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57caef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Now, import the necessary custom functions from the scripts\n",
    "from src.scripts.df_generator import get_dataset_directory, check_directory_exists, load_or_cache_dataframes, show_loaded_dfs\n",
    "from src.scripts.df_metadata import display_metadata_dfs, create_metadata_dfs, enrich_metadata_df\n",
    "from src.scripts.fetch_data_fields import fetch_and_compare_data_fields\n",
    "from src.scripts.build_data_fields_config import build_data_fields_config\n",
    "from src.scripts.df_filtering import filter_metadata_and_dataframes, process_dataframe\n",
    "from src.scripts.df_fuzzywuzzy import fuzzy_dataframe\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "os.chdir(os.path.join(os.getcwd(), 'src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767e2eb",
   "metadata": {},
   "source": [
    "## Step 2: Define and Check Dataset Directory\n",
    "\n",
    "Define the dataset directory and ensure it exists. This step is crucial as it sets the working directory for subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d55877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory found: c:\\Users\\Shiva\\Documents\\Data Scientist\\Mission 3 - Preparer des donn√©es pour un organismer de sante publique\\src\\dataset\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset directory\n",
    "notebook_directory =os.getcwd()  # This points to the root where the notebook is\n",
    "dataset_directory = os.path.join(notebook_directory, 'dataset')\n",
    "\n",
    "# Check if the dataset directory exists\n",
    "if not check_directory_exists(dataset_directory):\n",
    "    print(f\"Error: Directory '{dataset_directory}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Dataset directory found: {dataset_directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf42d6",
   "metadata": {},
   "source": [
    "## Step 3: Load and Cache DataFrames\n",
    "\n",
    "Load the data from the dataset directory into pandas DataFrames. The data can be loaded from cache or directly from the source files if the cache is not available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f4f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'fr.openfoodfacts.org.products' from cache.\n",
      "Nullity matrix for 'fr.openfoodfacts.org.products' has been generated.\n",
      "Loaded DataFrames: ['fr.openfoodfacts.org.products']\n",
      "Currently loaded DataFrames:\n",
      "DataFrame for file 'fr.openfoodfacts.org.products (320767, 146)':\n",
      "            code                                                url                     creator   created_t      created_datetime  ... collagen-meat-protein-ratio_100g cocoa_100g carbon-footprint_100g nutrition-score-fr_100g nutrition-score-uk_100g\n",
      "0  0000000003087  http://world-fr.openfoodfacts.org/produit/0000...  openfoodfacts-contributors  1474103866  2016-09-17T09:17:46Z  ...                              NaN        NaN                   NaN                     NaN                     NaN\n",
      "1  0000000004530  http://world-fr.openfoodfacts.org/produit/0000...             usda-ndb-import  1489069957  2017-03-09T14:32:37Z  ...                              NaN        NaN                   NaN                    14.0                    14.0\n",
      "2  0000000004559  http://world-fr.openfoodfacts.org/produit/0000...             usda-ndb-import  1489069957  2017-03-09T14:32:37Z  ...                              NaN        NaN                   NaN                     0.0                     0.0\n",
      "3  0000000016087  http://world-fr.openfoodfacts.org/produit/0000...             usda-ndb-import  1489055731  2017-03-09T10:35:31Z  ...                              NaN        NaN                   NaN                    12.0                    12.0\n",
      "4  0000000016094  http://world-fr.openfoodfacts.org/produit/0000...             usda-ndb-import  1489055653  2017-03-09T10:34:13Z  ...                              NaN        NaN                   NaN                     NaN                     NaN\n",
      "\n",
      "[5 rows x 146 columns]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Directory to store cached DataFrames\n",
    "CACHE_DIR = os.path.join(notebook_directory, 'data', 'cache') \n",
    "\n",
    "# Optionally, you can define a list of specific files to process\n",
    "specific_files = ['fr.openfoodfacts.org.products.csv']  # Set to None to process all files\n",
    "\n",
    "# Load DataFrames from cache or source files\n",
    "dfs = load_or_cache_dataframes(dataset_directory, CACHE_DIR, file_list=specific_files, separator='\\t')\n",
    "\n",
    "# Check if DataFrames are loaded\n",
    "if not dfs:\n",
    "    print(\"No DataFrames were loaded. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded DataFrames: {list(dfs.keys())}\")\n",
    "    show_loaded_dfs(dfs, df_names=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9756",
   "metadata": {},
   "source": [
    "## Step 4: Create and Display Metadata\n",
    "\n",
    "Generate metadata for the loaded DataFrames and display it to understand the structure and content of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21720f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Metadata DataFrames: ['fr.openfoodfacts.org.products']\n",
      "Metadata for fr.openfoodfacts.org.products (146, 8):\n",
      "                          Column Name    Dtype      Type  Fill Percentage  NaN Percentage  Bad Null Percentage  Duplicate Percentage  Missing Percentage\n",
      "0                                code   object   str(41)        99.992830        0.007170                  0.0              0.006859            0.007170\n",
      "1                                 url   object  str(652)        99.992830        0.007170                  0.0              0.006859            0.007170\n",
      "2                             creator   object   str(68)        99.999376        0.000624                  0.0             98.897642            0.000624\n",
      "3                           created_t   object   str(37)        99.999065        0.000935                  0.0             40.901651            0.000935\n",
      "4                    created_datetime   object   str(25)        99.997194        0.002806                  0.0             40.901340            0.002806\n",
      "..                                ...      ...       ...              ...             ...                  ...                   ...                 ...\n",
      "141  collagen-meat-protein-ratio_100g  float64  float(4)         0.051439       99.948561                  0.0             99.997818           99.948561\n",
      "142                        cocoa_100g  float64  float(5)         0.295542       99.704458                  0.0             99.973501           99.704458\n",
      "143             carbon-footprint_100g  float64  float(9)         0.083550       99.916450                  0.0             99.936714           99.916450\n",
      "144           nutrition-score-fr_100g  float64  float(5)        68.961583       31.038417                  0.0             99.982542           31.038417\n",
      "145           nutrition-score-uk_100g  float64  float(5)        68.961583       31.038417                  0.0             99.982542           31.038417\n",
      "\n",
      "[146 rows x 8 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create metadata DataFrames\n",
    "metadata_dfs = create_metadata_dfs(dfs)\n",
    "\n",
    "# Check if metadata DataFrames were created\n",
    "if not metadata_dfs:\n",
    "    print(\"No metadata DataFrames were created. Exiting.\")\n",
    "else:\n",
    "    print(f\"Created Metadata DataFrames: {list(metadata_dfs.keys())}\")\n",
    "    display_metadata_dfs(metadata_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0892c03",
   "metadata": {},
   "source": [
    "## Step 5: Fetch, Compare, and Configure Data Fields\n",
    "\n",
    "Fetch and compare data fields from the dataset, and build the necessary configuration files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7365c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HISTORY_DIR: True\n",
      "Created DIFF_DIR: True\n",
      "No changes detected on data\\data_fields.txt.\n",
      "Config file 'data_fields_config.json' has been updated and saved.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(notebook_directory,'data')\n",
    "\n",
    "# Run the fetch and compare data fields script\n",
    "fetch_and_compare_data_fields(DATA_DIR)\n",
    "\n",
    "# Build the config file\n",
    "build_data_fields_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78565870",
   "metadata": {},
   "source": [
    "## Step 6: Filter and Process DataFrames\n",
    "\n",
    "Filter the metadata and corresponding DataFrames, and save the filtered data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8bae2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined metadata (146, 11) has been saved or updated.\n"
     ]
    }
   ],
   "source": [
    "# Load the config.json\n",
    "script_dir = os.path.join(notebook_directory,'scritps')\n",
    "config_path = os.path.join(notebook_directory, 'config', 'data_fields_config.json')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Enrich the metadata DataFrame\n",
    "combined_metadata = pd.concat(metadata_dfs.values(), keys=metadata_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'DataFrame'})\n",
    "combined_metadata = enrich_metadata_df(combined_metadata, config)\n",
    "\n",
    "\n",
    "# Save the combined metadata DataFrame to a CSV file\n",
    "output_dir = os.path.join(notebook_directory, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "combined_metadata_path = os.path.join(output_dir, 'combined_metadata.csv')\n",
    "combined_metadata.to_csv(combined_metadata_path, index=False)\n",
    "print(f\"Combined metadata {combined_metadata.shape} has been saved or updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1e66d4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run_dash_app() takes 0 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m dfs \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# Load your actual DataFrames if needed\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Run the Dash app\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mrun_dash_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: run_dash_app() takes 0 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from src.scripts.plot_metadata_clusters import run_dash_app\n",
    "\n",
    "# Load or prepare your data\n",
    "output_dir = os.path.join(notebook_directory, 'data')\n",
    "combined_metadata_path = os.path.join(output_dir, 'combined_metadata.csv')\n",
    "combined_metadata = pd.read_csv(combined_metadata_path)\n",
    "\n",
    "dfs = {}  # Load your actual DataFrames if needed\n",
    "\n",
    "# Run the Dash app\n",
    "run_dash_app(combined_metadata, dfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
