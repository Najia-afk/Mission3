{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd399c4d",
   "metadata": {},
   "source": [
    "# Data Processing and Metadata Enrichment Pipeline\n",
    "\n",
    "This notebook guides you through a step-by-step process for loading, processing, and analyzing a dataset using a combination of custom scripts. The workflow includes loading data, creating metadata, filtering data, and performing fuzzy matching.\n",
    "\n",
    "## Steps:\n",
    "1. Set up environment and import necessary modules.\n",
    "2. Define and check dataset directory.\n",
    "3. Load and cache dataframes.\n",
    "4. Create and display metadata.\n",
    "5. Fetch, compare, and configure data fields.\n",
    "6. Filter and process dataframes.\n",
    "7. Perform fuzzy matching.\n",
    "8. Save processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43befc5",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "We begin by importing the necessary libraries and functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57caef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Now, import the necessary custom functions from the scripts\n",
    "\n",
    "from src.scripts.df_metadata import display_metadata_dfs, create_metadata_dfs, enrich_metadata_df\n",
    "from src.scripts.fetch_data_fields import fetch_and_compare_data_fields\n",
    "from src.scripts.build_data_fields_config import build_data_fields_config\n",
    "\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "os.chdir(os.path.join(os.getcwd(), 'src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767e2eb",
   "metadata": {},
   "source": [
    "## Step 2: Define and Check Dataset Directory\n",
    "\n",
    "Define the dataset directory and ensure it exists. This step is crucial as it sets the working directory for subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d55877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory found\n"
     ]
    }
   ],
   "source": [
    "from src.scripts.df_generator import check_directory_exists, load_or_cache_dataframes, show_loaded_dfs\n",
    "# Define the dataset directory\n",
    "notebook_directory =os.getcwd()  # This points to the root where the notebook is\n",
    "dataset_directory = os.path.join(notebook_directory, 'dataset')\n",
    "\n",
    "# Check if the dataset directory exists\n",
    "if not check_directory_exists(dataset_directory):\n",
    "    print(f\"Error: Directory dataset does not exist.\")\n",
    "else:\n",
    "    print(f\"Dataset directory found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf42d6",
   "metadata": {},
   "source": [
    "## Step 3: Load and Cache DataFrames\n",
    "\n",
    "Load the data from the dataset directory into pandas DataFrames. The data can be loaded from cache or directly from the source files if the cache is not available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f4f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'fr.openfoodfacts.org.products' from cache.\n",
      "Nullity matrix for 'fr.openfoodfacts.org.products' has been generated.\n",
      "Loaded DataFrames: ['fr.openfoodfacts.org.products']\n",
      "Currently loaded DataFrames:\n",
      "DataFrame for file 'fr.openfoodfacts.org.products (320767, 146)':\n",
      "            code                                                url  ... nutrition-score-fr_100g nutrition-score-uk_100g\n",
      "0  0000000003087  http://world-fr.openfoodfacts.org/produit/0000...  ...                     NaN                     NaN\n",
      "1  0000000004530  http://world-fr.openfoodfacts.org/produit/0000...  ...                    14.0                    14.0\n",
      "2  0000000004559  http://world-fr.openfoodfacts.org/produit/0000...  ...                     0.0                     0.0\n",
      "3  0000000016087  http://world-fr.openfoodfacts.org/produit/0000...  ...                    12.0                    12.0\n",
      "4  0000000016094  http://world-fr.openfoodfacts.org/produit/0000...  ...                     NaN                     NaN\n",
      "\n",
      "[5 rows x 146 columns]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Directory to store cached DataFrames\n",
    "CACHE_DIR = os.path.join(notebook_directory, 'data', 'cache') \n",
    "\n",
    "# Optionally, you can define a list of specific files to process\n",
    "specific_files = ['fr.openfoodfacts.org.products.csv']  # Set to None to process all files\n",
    "\n",
    "# Load DataFrames from cache or source files\n",
    "dfs = load_or_cache_dataframes(dataset_directory, CACHE_DIR, file_list=specific_files, separator='\\t')\n",
    "\n",
    "# Check if DataFrames are loaded\n",
    "if not dfs:\n",
    "    print(\"No DataFrames were loaded. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded DataFrames: {list(dfs.keys())}\")\n",
    "    show_loaded_dfs(dfs, df_names=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9756",
   "metadata": {},
   "source": [
    "## Step 4: Create and Display Metadata\n",
    "\n",
    "Generate metadata for the loaded DataFrames and display it to understand the structure and content of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21720f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Metadata DataFrames: ['fr.openfoodfacts.org.products']\n",
      "Metadata for fr.openfoodfacts.org.products (146, 8):\n",
      "                          Column Name    Dtype  ... Duplicate Percentage  Missing Percentage\n",
      "0                                code   object  ...             0.000000            0.007170\n",
      "1                                 url   object  ...             0.000000            0.007170\n",
      "2                             creator   object  ...            98.897947            0.000624\n",
      "3                           created_t   object  ...            40.901410            0.000935\n",
      "4                    created_datetime   object  ...            40.899993            0.002806\n",
      "..                                ...      ...  ...                  ...                 ...\n",
      "141  collagen-meat-protein-ratio_100g  float64  ...            96.363636           99.948561\n",
      "142                        cocoa_100g  float64  ...            91.139241           99.704458\n",
      "143             carbon-footprint_100g  float64  ...            24.626866           99.916450\n",
      "144           nutrition-score-fr_100g  float64  ...            99.975136           31.038417\n",
      "145           nutrition-score-uk_100g  float64  ...            99.975136           31.038417\n",
      "\n",
      "[146 rows x 8 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create metadata DataFrames\n",
    "metadata_dfs = create_metadata_dfs(dfs)\n",
    "\n",
    "# Check if metadata DataFrames were created\n",
    "if not metadata_dfs:\n",
    "    print(\"No metadata DataFrames were created. Exiting.\")\n",
    "else:\n",
    "    print(f\"Created Metadata DataFrames: {list(metadata_dfs.keys())}\")\n",
    "    display_metadata_dfs(metadata_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0892c03",
   "metadata": {},
   "source": [
    "## Step 5: Fetch, Compare, and Configure Data Fields\n",
    "\n",
    "Fetch and compare data fields from the dataset, and build the necessary configuration files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7365c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HISTORY_DIR: True\n",
      "Created DIFF_DIR: True\n",
      "No changes detected on data\\data_fields.txt.\n",
      "Config file 'data_fields_config.json' has been updated and saved.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(notebook_directory,'data')\n",
    "\n",
    "# Run the fetch and compare data fields script\n",
    "fetch_and_compare_data_fields(DATA_DIR)\n",
    "\n",
    "# Build the config file\n",
    "build_data_fields_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78565870",
   "metadata": {},
   "source": [
    "## Step 6: Filter and Process DataFrames\n",
    "\n",
    "Filter the metadata and corresponding DataFrames, and save the filtered data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8bae2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined metadata (146, 11) has been saved or updated.\n"
     ]
    }
   ],
   "source": [
    "# Load the config.json\n",
    "script_dir = os.path.join(notebook_directory,'scritps')\n",
    "config_path = os.path.join(notebook_directory, 'config', 'data_fields_config.json')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Enrich the metadata DataFrame\n",
    "combined_metadata = pd.concat(metadata_dfs.values(), keys=metadata_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'DataFrame'})\n",
    "combined_metadata = enrich_metadata_df(combined_metadata, config)\n",
    "\n",
    "\n",
    "# Save the combined metadata DataFrame to a CSV file\n",
    "output_dir = os.path.join(notebook_directory, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "combined_metadata_path = os.path.join(output_dir, 'combined_metadata.csv')\n",
    "combined_metadata.to_csv(combined_metadata_path, index=False)\n",
    "print(f\"Combined metadata {combined_metadata.shape} has been saved or updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fd368",
   "metadata": {},
   "source": [
    "## Step 7: Identify columns cluster\n",
    "\n",
    "Checking cluster of columns based on Duplicate(%) and Fill(%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e66d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_metadata_clusters import run_dash_app\n",
    "\n",
    "combined_metadata_cluster = combined_metadata.copy()\n",
    "\n",
    "# Run the Dash app  \n",
    "run_dash_app(combined_metadata_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e40ee",
   "metadata": {},
   "source": [
    "## Step 8: Analyse, fuzzy and other stuff\n",
    "\n",
    "blabla\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c62065b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x27b42c21310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame 'fr.openfoodfacts.org.products' to retain only relevant columns.\n",
      "Check the nutrition combination file for more details about fields frequency.\n",
      "Check the pnns_groups combination file for more details about fields frequency.\n",
      "[nutrition] Total combinations to process: 441\n",
      "[nutrition] Finished processing all combinations.\n",
      "Grouped results for nutrition have been generated\n",
      "[pnns_groups] Total combinations to process: 44\n",
      "[pnns_groups] Finished processing all combinations.\n",
      "Grouped results for pnns_groups have been generated\n",
      "Fuzzy matching and grouping completed for all checks.\n",
      "Processed DataFrame 'fr.openfoodfacts.org.products' and metadata have been saved.\n"
     ]
    }
   ],
   "source": [
    "from src.scripts.df_filtering import filter_metadata_and_dataframes, process_dataframe\n",
    "from src.scripts.df_fuzzywuzzy import fuzzy_dataframe\n",
    "\n",
    "\n",
    "# Specify your datetime checks as a list of tuples\n",
    "datetime_checks = [\n",
    "    # ('created_t', 'created_datetime'),\n",
    "    # ('last_modified_t', 'last_modified_datetime')\n",
    "]\n",
    "\n",
    "# Specify your field frequency checks as a list of tuples\n",
    "field_checks = [\n",
    "    #(['countries', 'countries_tags', 'countries_fr'], 'countries'),\n",
    "    #(['ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n'], 'ingredients_palm_oil'),\n",
    "    (['nutrition_grade_fr', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g'], 'nutrition'),\n",
    "    #(['brands_tags', 'brands'], 'brands'),\n",
    "    #(['additives_n', 'additives', 'additives_tags', 'additives_fr'], 'additives'),\n",
    "    #(['states', 'states_tags', 'states_fr'], 'states'),\n",
    "    (['pnns_groups_1', 'pnns_groups_2'], 'pnns_groups')\n",
    "    \n",
    "]\n",
    "\n",
    "# Columns to check for at least one non-null value\n",
    "columns_to_check = [\n",
    "    'nutrition_grade_fr', 'energy_100g', 'fat_100g', 'saturated-fat_100g',\n",
    "    'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g',\n",
    "    'fiber_100g', 'proteins_100g', 'salt_100g', 'sodium_100g', 'vitamin-a_100g',\n",
    "    'vitamin-c_100g', 'calcium_100g', 'iron_100g', 'nutrition-score-fr_100g',\n",
    "    'nutrition-score-uk_100g'\n",
    "]\n",
    "\n",
    "# Fields to be deleted after anaylysis\n",
    "fields_to_delete = [\n",
    "    'url', 'created_t', 'created_datetime','last_modified_t', 'last_modified_datetime', 'states', 'states_tags', 'states_fr', 'countries', 'countries_tags', 'countries_fr',\n",
    "    'brands_tags', 'brands', 'additives_n', 'additives', 'additives_tags', 'additives_fr',\n",
    "    'creator','ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n',\n",
    "    'quantity', 'serving_size', 'additives', 'ingredients_text','product_name',\n",
    "    'categories','categories_tags','categories_fr','packaging','packaging_tags','image_url','image_small_url','main_category','main_category_fr'\n",
    "    ]\n",
    "\n",
    "# Now process your specific DataFrame\n",
    "df_name = 'fr.openfoodfacts.org.products'\n",
    "\n",
    "# Filter and process DataFrame in one step\n",
    "if df_name in dfs:\n",
    "    combined_metadata, filtered_dfs = filter_metadata_and_dataframes(combined_metadata, dfs, 20)\n",
    "    process_dataframe(filtered_dfs[df_name], log_dir='logs', temp_dir='temp', datetime_checks=datetime_checks, field_checks=field_checks)\n",
    "    fuzzy_dataframe(temp_dir='temp', config_dir='config', checks=field_checks, threshold=90)\n",
    "\n",
    "    # Drop rows where all specified columns are null\n",
    "    filtered_dfs[df_name].dropna(subset=columns_to_check, how='all', inplace=True)\n",
    "\n",
    "    # Drop duplicates after filtering rows\n",
    "    filtered_dfs[df_name].drop_duplicates(inplace=True)\n",
    "\n",
    "    # Delete the specified columns from combined_metadata and update the related DataFrame\n",
    "    combined_metadata = combined_metadata[~combined_metadata['Column Name'].isin(fields_to_delete)]\n",
    "    filtered_dfs[df_name] = filtered_dfs[df_name][combined_metadata['Column Name']]\n",
    "\n",
    "    # Save the processed DataFrame to the dataset directory\n",
    "    dataset_path = os.path.join('dataset', f'processed_{df_name}.csv')\n",
    "    filtered_dfs[df_name].to_csv(dataset_path, index=False)\n",
    "    \n",
    "    # Save the updated metadata to the data directory\n",
    "    metadata_path = os.path.join('data', f'processed_metadata.csv')\n",
    "    combined_metadata.to_csv(metadata_path, index=False)\n",
    "    \n",
    "    print(f\"Processed DataFrame '{df_name}' and metadata have been saved.\")\n",
    "else:\n",
    "    print(f\"DataFrame '{df_name}' not found in the loaded DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2df568",
   "metadata": {},
   "source": [
    "## Step 9: Nutrition score Clustering Dashboard\n",
    "\n",
    "blabla\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ea5846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter Notebook\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8051/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x27b4414bc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.scripts.plot_nutriscore import run_dash_or_show_iframe, safe_eval\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "# os.chdir(os.path.join(os.getcwd(), 'src'))\n",
    "# notebook_directory =os.getcwd()\n",
    "\n",
    "# Assuming notebook_directory is already defined in your notebook\n",
    "nutriscore_directory = os.path.join(notebook_directory, 'temp', 'nutrition_combination_log.csv')\n",
    "nutriscore = pd.read_csv(nutriscore_directory)\n",
    "\n",
    "run_dash_or_show_iframe(nutriscore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87eb0f",
   "metadata": {},
   "source": [
    "### Nutrient Maximum Limits Justification\n",
    "\n",
    "| Nutrient                      | Maximum Limit      | Justification |\n",
    "|-------------------------------|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Energy (energy_100g)**       | 950 kcal/100g      | The upper limit of 950 kcal per 100g accounts for extremely energy-dense foods like pure oils and concentrated products, while capturing potential data entry errors without excluding valid outliers.                                                                                                       |\n",
    "| **Fat (fat_100g)**             | 95g/100g           | While pure fat can reach 100g/100g, lowering the limit slightly to 95g/100g flags potential rounding errors in data entry, as it's rare for foods to contain exactly 100g of fat.                                                                                                                             |\n",
    "| **Saturated Fat (saturated-fat_100g)** | 55g/100g  | High-saturated fat products like butter can have up to 50-60% saturated fat. A limit of 55g/100g allows flexibility for processed fats while still flagging extreme cases.                                                                                                                                   |\n",
    "| **Carbohydrates (carbohydrates_100g)** | 95g/100g | Carbohydrates can theoretically reach 100% of a food's weight, but setting the limit at 95g/100g helps to flag data entry errors while accommodating foods with high carbohydrate content.                                                                                                                  |\n",
    "| **Sugars (sugars_100g)**       | 95g/100g           | Sugars, although able to reach 100g/100g, are rarely that high in practice. Setting the limit at 95g/100g captures realistic values while identifying potential overstatements.                                                                                                                               |\n",
    "| **Sodium (sodium_100g)**       | 3g/100g            | While most foods don't exceed 2.3g/100g, certain salt-heavy products like salted meats or fish can reach higher sodium levels. A 3g/100g limit captures these outliers while maintaining realistic boundaries.                                                                                                 |\n",
    "| **Salt (salt_100g)**           | 6g/100g            | With sodium reaching 3g/100g in some extreme cases, the corresponding salt content would be around 6g/100g, maintaining logical sodium-salt relationships for highly salted products.                                                                                                                         |\n",
    "| **Trans Fat (trans-fat_100g)**  | 5g/100g            | Modern food regulations limit trans fats in many countries, making it rare for foods to exceed 5g/100g. This lower limit ensures compliance with current guidelines and excludes unrealistic trans fat levels.                                                                                                |\n",
    "| **Cholesterol (cholesterol_100g)** | 500mg/100g     | High-cholesterol foods like organ meats are accommodated, but a higher limit of 500mg/100g better captures naturally high-cholesterol foods without excluding legitimate entries.                                                                                                                             |\n",
    "| **Fiber (fiber_100g)**         | 50g/100g           | Fiber content can be high in foods like bran, but a limit of 50g/100g ensures that even fiber-dense products are realistically capped, filtering out unrealistic entries.                                                                                                                                     |\n",
    "| **Proteins (proteins_100g)**   | 90g/100g           | High-protein products, especially supplements, can reach up to 90g/100g. This limit allows for protein-dense foods while filtering out implausible data entries.                                                                                                                                              |\n",
    "| **Vitamin A (vitamin-a_100g)** | 30mg/100g          | Foods like liver can contain high levels of Vitamin A, but 30mg/100g is a more conservative upper limit to ensure that extreme, potentially toxic levels are flagged as data errors.                                                                                                                           |\n",
    "| **Vitamin C (vitamin-c_100g)** | 50mg/100g          | While some fruits have high Vitamin C concentrations, a 50mg/100g limit is sufficient to capture natural sources while identifying improbable values.                                                                                                                                                         |\n",
    "| **Calcium (calcium_100g)**     | 30mg/100g          | Although fortified foods may exceed natural calcium levels, 30mg/100g is a reasonable limit that captures high-calcium foods while excluding artificially inflated entries.                                                                                                                                     |\n",
    "| **Iron (iron_100g)**           | 40mg/100g          | Iron-rich foods like red meat and fortified cereals are accommodated, but a 40mg/100g limit is more realistic for naturally occurring iron levels, preventing data entry errors.                                                                                                                               |\n",
    "\n",
    "### Additional Justification:\n",
    "- **Nutritional Guidelines**: Limits are based on standard nutritional data from sources such as USDA, EFSA, and general dietary recommendations.\n",
    "- **Data Integrity**: These limits ensure data is free from common errors (e.g., mistyping, incorrect unit conversions), helping to maintain clean, reliable data for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f1bcd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory found\n",
      "Loaded 'processed_fr.openfoodfacts.org.products' from cache.\n",
      "Nullity matrix for 'processed_fr.openfoodfacts.org.products' has been generated.\n",
      "Loaded DataFrames: ['processed_fr.openfoodfacts.org.products']\n",
      "Currently loaded DataFrames:\n",
      "DataFrame for file 'processed_fr.openfoodfacts.org.products (262828, 21)':\n",
      "            code nutrition_grade_fr  ... nutrition-score-fr_100g nutrition-score-uk_100g\n",
      "0  0000000004530                  d  ...                    14.0                    14.0\n",
      "1  0000000004559                  b  ...                     0.0                     0.0\n",
      "2  0000000016087                  d  ...                    12.0                    12.0\n",
      "3  0000000016094                NaN  ...                     NaN                     NaN\n",
      "4  0000000016100                NaN  ...                     NaN                     NaN\n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "\n",
      "pnns_groups_1 and pnns_groups_2 combinations have been standardized.\n",
      "Processed DataFrame 'processed_fr.openfoodfacts.org.products' has been updated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.scripts.df_generator import check_directory_exists, load_or_cache_dataframes, show_loaded_dfs\n",
    "from src.scripts.df_business_data_integrity import run_integrity_check\n",
    "from src.scripts.df_pnns_group import standardize_pnns_groups\n",
    "from src.scripts.df_nutriscore import check_and_standardize_nutrition_grades\n",
    "\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "\n",
    "# Define the dataset directory\n",
    "notebook_directory = os.getcwd()  # This points to the root where the notebook is\n",
    "dataset_directory = os.path.join(notebook_directory, 'dataset')\n",
    "json_file_path = os.path.join(notebook_directory, 'config', 'pnns_groups_grouped_results.json')\n",
    "\n",
    "# Check if the dataset directory exists\n",
    "if not check_directory_exists(dataset_directory):\n",
    "    print(f\"Error: Directory dataset does not exist.\")\n",
    "else:\n",
    "    print(f\"Dataset directory found\")\n",
    "\n",
    "# Directory to store cached DataFrames\n",
    "CACHE_DIR = os.path.join(notebook_directory, 'data', 'cache')\n",
    "\n",
    "# Optionally, you can define a list of specific files to process\n",
    "specific_files = ['processed_fr.openfoodfacts.org.products.csv']\n",
    "\n",
    "# Load DataFrames from cache or source files\n",
    "processed_dfs = load_or_cache_dataframes(dataset_directory, CACHE_DIR, file_list=specific_files, separator=',')\n",
    "\n",
    "# Check if DataFrames are loaded\n",
    "if not processed_dfs:\n",
    "    print(\"No DataFrames were loaded. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded DataFrames: {list(processed_dfs.keys())}\")\n",
    "    show_loaded_dfs(processed_dfs, df_names=None)\n",
    "\n",
    "# Get the specific DataFrame for processing\n",
    "df_name = 'processed_fr.openfoodfacts.org.products'\n",
    "if df_name in processed_dfs:\n",
    "    df = processed_dfs[df_name]  # Pass the DataFrame directly if not a dictionary\n",
    "    run_integrity_check(df, log_dir='logs')\n",
    "    standardize_pnns_groups(json_file_path) \n",
    "    check_and_standardize_nutrition_grades(df)\n",
    "    \n",
    "    # Save the processed DataFrame to the dataset directory\n",
    "    dataset_path = os.path.join('dataset', f'standardized_{df_name}.csv')\n",
    "    processed_dfs[df_name].to_csv(dataset_path, index=False)\n",
    "    print(f\"Processed DataFrame '{df_name}' has been updated.\")\n",
    "else:\n",
    "    print(f\"DataFrame '{df_name}' not found in the loaded DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a4bdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'standardized_processed_fr.openfoodfacts.org.products' from cache.\n",
      "Nullity matrix for 'standardized_processed_fr.openfoodfacts.org.products' has been generated.\n",
      "Loaded DataFrames: ['standardized_processed_fr.openfoodfacts.org.products']\n",
      "Currently loaded DataFrames:\n",
      "DataFrame for file 'standardized_processed_fr.openfoodfacts.org.products (262828, 21)':\n",
      "            code nutrition_grade_fr  ... nutrition-score-fr_100g nutrition-score-uk_100g\n",
      "0  0000000004530                  d  ...                    14.0                    14.0\n",
      "1  0000000004559                  b  ...                     0.0                     0.0\n",
      "2  0000000016087                  d  ...                    12.0                    12.0\n",
      "3  0000000016094                NaN  ...                     NaN                     NaN\n",
      "4  0000000016100                NaN  ...                     NaN                     NaN\n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "\n",
      "Created Metadata DataFrames: ['standardized_processed_fr.openfoodfacts.org.products']\n",
      "Metadata for standardized_processed_fr.openfoodfacts.org.products (262828, 21):\n",
      "                 code nutrition_grade_fr  ... nutrition-score-fr_100g nutrition-score-uk_100g\n",
      "0       0000000004530                  d  ...                    14.0                    14.0\n",
      "1       0000000004559                  b  ...                     0.0                     0.0\n",
      "2       0000000016087                  d  ...                    12.0                    12.0\n",
      "3       0000000016094                NaN  ...                     NaN                     NaN\n",
      "4       0000000016100                NaN  ...                     NaN                     NaN\n",
      "...               ...                ...  ...                     ...                     ...\n",
      "262823         989898                NaN  ...                     NaN                     NaN\n",
      "262824  9900000000233                  b  ...                     0.0                     0.0\n",
      "262825       99111250                  c  ...                     2.0                     0.0\n",
      "262826       99567453                  b  ...                     0.0                     0.0\n",
      "262827   999990026839                NaN  ...                     NaN                     NaN\n",
      "\n",
      "[262828 rows x 21 columns]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optionally, you can define a list of specific files to process\n",
    "specific_files = ['standardized_processed_fr.openfoodfacts.org.products.csv']  # Set to None to process all files\n",
    "\n",
    "# Load DataFrames from cache or source files\n",
    "dfs_std = load_or_cache_dataframes(dataset_directory, CACHE_DIR, file_list=specific_files, separator=',')\n",
    "\n",
    "# Check if DataFrames are loaded\n",
    "if not dfs_std:\n",
    "    print(\"No DataFrames were loaded. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded DataFrames: {list(dfs_std.keys())}\")\n",
    "    show_loaded_dfs(dfs_std, df_names=None)\n",
    "\n",
    "\n",
    "# Check if metadata DataFrames were created\n",
    "if not dfs_std:\n",
    "    print(\"No metadata DataFrames were created. Exiting.\")\n",
    "else:\n",
    "    print(f\"Created Metadata DataFrames: {list(dfs_std.keys())}\")\n",
    "    display_metadata_dfs(dfs_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21958e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the run_dash_app function from the module\n",
    "from src.scripts.plot_imputation import run_dash_app\n",
    "\n",
    "# Pass your dataframe to the Dash app\n",
    "#run_dash_app(dfs_std['standardized_processed_fr.openfoodfacts.org.products'])\n",
    "#dfs_std['standardized_processed_fr.openfoodfacts.org.products'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c327980c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24252\\2931422436.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomplete_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# Store the model for later use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Documents\\Data Scientist\\Mission 3 - Preparer des données pour un organismer de sante publique\\mission3_venv\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1469\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1470\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1473\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Shiva\\Documents\\Data Scientist\\Mission 3 - Preparer des données pour un organismer de sante publique\\mission3_venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    364\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Documents\\Data Scientist\\Mission 3 - Preparer des données pour un organismer de sante publique\\mission3_venv\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Documents\\Data Scientist\\Mission 3 - Preparer des données pour un organismer de sante publique\\mission3_venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1297\u001b[0m         raise ValueError(\n\u001b[0;32m   1298\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mestimator_name\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m requires y to be passed, but the target y is None\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1302\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Documents\\Data Scientist\\Mission 3 - Preparer des données pour un organismer de sante publique\\mission3_venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m                 raise ValueError(\n\u001b[0;32m   1015\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m                 \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Documents\\Data Scientist\\Mission 3 - Preparer des données pour un organismer de sante publique\\mission3_venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Documents\\Data Scientist\\Mission 3 - Preparer des données pour un organismer de sante publique\\mission3_venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'd'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data\n",
    "df = dfs_std['standardized_processed_fr.openfoodfacts.org.products']\n",
    "\n",
    "# Handle missing values: Keep track of which values are missing\n",
    "missing_mask_energy = df['energy_100g'].isna()\n",
    "missing_mask_fat = df['fat_100g'].isna()\n",
    "\n",
    "# Convert categorical variables to string (if not already)\n",
    "df['pnns_groups_1'] = df['pnns_groups_1'].astype(str)\n",
    "df['pnns_groups_2'] = df['pnns_groups_2'].astype(str)\n",
    "\n",
    "# Encode categorical variables using OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\n",
    "\n",
    "# Train separate models for pnns_groups_1 and pnns_groups_2\n",
    "encoded_categories = encoder.fit_transform(df[['pnns_groups_1', 'pnns_groups_2']])\n",
    "encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out())\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "# Drop original categorical columns to prevent mixing types\n",
    "df.drop(columns=['pnns_groups_1', 'pnns_groups_2'], inplace=True)\n",
    "\n",
    "# Define features\n",
    "features = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'trans-fat_100g',\n",
    "            'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g',\n",
    "            'proteins_100g', 'salt_100g', 'sodium_100g', 'vitamin-a_100g',\n",
    "            'vitamin-c_100g', 'calcium_100g', 'iron_100g']\n",
    "\n",
    "# Step 1: Train models on non-missing data for each feature\n",
    "predictions = {}\n",
    "\n",
    "for feature in features:\n",
    "    # Use only rows where the target feature is not NaN\n",
    "    complete_data = df.dropna(subset=[feature])\n",
    "    \n",
    "    if complete_data.empty:  # Skip if there are no complete rows\n",
    "        continue\n",
    "    \n",
    "    X = complete_data.drop(columns=[feature])\n",
    "    y = complete_data[feature]\n",
    "    \n",
    "    # Ensure all features are numeric\n",
    "    X = X.select_dtypes(include=['number'])  # Keep only numeric columns\n",
    "    \n",
    "    # Train the model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Store the model for later use\n",
    "    predictions[feature] = model\n",
    "\n",
    "# Step 2: Predict missing values using the trained models\n",
    "def predict_missing_values(df, predictions):\n",
    "    for feature, model in predictions.items():\n",
    "        missing_mask = df[feature].isna()\n",
    "        if missing_mask.any():\n",
    "            X_missing = df[missing_mask].drop(columns=[feature])\n",
    "            # Ensure all features are numeric\n",
    "            X_missing = X_missing.select_dtypes(include=['number'])  # Keep only numeric columns\n",
    "            # Predict missing values\n",
    "            df.loc[missing_mask, feature] = model.predict(X_missing)\n",
    "    return df\n",
    "\n",
    "# Fill in missing values iteratively\n",
    "df = predict_missing_values(df, predictions)\n",
    "\n",
    "# Step 3: Predict pnns_groups_1 based on features\n",
    "# Create a separate model for predicting pnns_groups_1\n",
    "X_pnns = df.dropna(subset=['pnns_groups_1'])\n",
    "y_pnns = X_pnns['pnns_groups_1']\n",
    "X_pnns = X_pnns.drop(columns=['pnns_groups_1'])\n",
    "\n",
    "# Ensure to encode categorical features if needed\n",
    "X_pnns = encoder.transform(X_pnns[['pnns_groups_1', 'pnns_groups_2']])\n",
    "X_pnns = pd.DataFrame(X_pnns, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Train the classifier\n",
    "pnns_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "pnns_model.fit(X_pnns, y_pnns)\n",
    "\n",
    "# Predict missing pnns_groups_1\n",
    "missing_pnns_mask = df['pnns_groups_1'].isna()\n",
    "if missing_pnns_mask.any():\n",
    "    X_missing_pnns = df[missing_pnns_mask].drop(columns=['pnns_groups_1'])\n",
    "    # Ensure to encode categorical features if needed\n",
    "    X_missing_pnns_encoded = encoder.transform(X_missing_pnns[['pnns_groups_1', 'pnns_groups_2']])\n",
    "    df.loc[missing_pnns_mask, 'pnns_groups_1'] = pnns_model.predict(X_missing_pnns_encoded)\n",
    "\n",
    "# Visualize the relationship between features and pnns_groups\n",
    "sns.scatterplot(data=df, x='energy_100g', y='fat_100g', hue='pnns_groups_1', alpha=0.7)\n",
    "plt.title('Energy vs Fat colored by PNNS Groups')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mission3_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
