{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd399c4d",
   "metadata": {},
   "source": [
    "# Data Processing and Metadata Enrichment Pipeline\n",
    "\n",
    "This notebook guides you through a step-by-step process for loading, processing, and analyzing a dataset using a combination of custom scripts. The workflow includes loading data, creating metadata, filtering data, and performing fuzzy matching.\n",
    "\n",
    "## Steps:\n",
    "1. Set up environment and import necessary modules.\n",
    "2. Define and check dataset directory.\n",
    "3. Load and cache dataframes.\n",
    "4. Create and display metadata.\n",
    "5. Fetch, compare, and configure data fields.\n",
    "6. Filter and process dataframes.\n",
    "7. Perform fuzzy matching.\n",
    "8. Save processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43befc5",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "We begin by importing the necessary libraries and functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57caef52",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Le fichier spécifié est introuvable: 'c:\\\\Git\\\\Mission3\\\\src\\\\src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_data_fields_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_data_fields_config\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Change the current working directory to 'src'\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msrc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Le fichier spécifié est introuvable: 'c:\\\\Git\\\\Mission3\\\\src\\\\src'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Now, import the necessary custom functions from the scripts\n",
    "from src.scripts.df_generator import get_dataset_directory, check_directory_exists, load_or_cache_dataframes, show_loaded_dfs\n",
    "from src.scripts.df_metadata import display_metadata_dfs, create_metadata_dfs, enrich_metadata_df\n",
    "from src.scripts.fetch_data_fields import fetch_and_compare_data_fields\n",
    "from src.scripts.build_data_fields_config import build_data_fields_config\n",
    "\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "os.chdir(os.path.join(os.getcwd(), 'src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767e2eb",
   "metadata": {},
   "source": [
    "## Step 2: Define and Check Dataset Directory\n",
    "\n",
    "Define the dataset directory and ensure it exists. This step is crucial as it sets the working directory for subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d55877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset directory\n",
    "notebook_directory =os.getcwd()  # This points to the root where the notebook is\n",
    "dataset_directory = os.path.join(notebook_directory, 'dataset')\n",
    "\n",
    "# Check if the dataset directory exists\n",
    "if not check_directory_exists(dataset_directory):\n",
    "    print(f\"Error: Directory '{dataset_directory}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Dataset directory found: {dataset_directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf42d6",
   "metadata": {},
   "source": [
    "## Step 3: Load and Cache DataFrames\n",
    "\n",
    "Load the data from the dataset directory into pandas DataFrames. The data can be loaded from cache or directly from the source files if the cache is not available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store cached DataFrames\n",
    "CACHE_DIR = os.path.join(notebook_directory, 'data', 'cache') \n",
    "\n",
    "# Optionally, you can define a list of specific files to process\n",
    "specific_files = ['fr.openfoodfacts.org.products.csv']  # Set to None to process all files\n",
    "\n",
    "# Load DataFrames from cache or source files\n",
    "dfs = load_or_cache_dataframes(dataset_directory, CACHE_DIR, file_list=specific_files, separator='\\t')\n",
    "\n",
    "# Check if DataFrames are loaded\n",
    "if not dfs:\n",
    "    print(\"No DataFrames were loaded. Exiting.\")\n",
    "else:\n",
    "    print(f\"Loaded DataFrames: {list(dfs.keys())}\")\n",
    "    show_loaded_dfs(dfs, df_names=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9756",
   "metadata": {},
   "source": [
    "## Step 4: Create and Display Metadata\n",
    "\n",
    "Generate metadata for the loaded DataFrames and display it to understand the structure and content of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21720f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata DataFrames\n",
    "metadata_dfs = create_metadata_dfs(dfs)\n",
    "\n",
    "# Check if metadata DataFrames were created\n",
    "if not metadata_dfs:\n",
    "    print(\"No metadata DataFrames were created. Exiting.\")\n",
    "else:\n",
    "    print(f\"Created Metadata DataFrames: {list(metadata_dfs.keys())}\")\n",
    "    display_metadata_dfs(metadata_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0892c03",
   "metadata": {},
   "source": [
    "## Step 5: Fetch, Compare, and Configure Data Fields\n",
    "\n",
    "Fetch and compare data fields from the dataset, and build the necessary configuration files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7365c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(notebook_directory,'data')\n",
    "\n",
    "# Run the fetch and compare data fields script\n",
    "fetch_and_compare_data_fields(DATA_DIR)\n",
    "\n",
    "# Build the config file\n",
    "build_data_fields_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78565870",
   "metadata": {},
   "source": [
    "## Step 6: Filter and Process DataFrames\n",
    "\n",
    "Filter the metadata and corresponding DataFrames, and save the filtered data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bae2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config.json\n",
    "script_dir = os.path.join(notebook_directory,'scritps')\n",
    "config_path = os.path.join(notebook_directory, 'config', 'data_fields_config.json')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Enrich the metadata DataFrame\n",
    "combined_metadata = pd.concat(metadata_dfs.values(), keys=metadata_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'DataFrame'})\n",
    "combined_metadata = enrich_metadata_df(combined_metadata, config)\n",
    "\n",
    "\n",
    "# Save the combined metadata DataFrame to a CSV file\n",
    "output_dir = os.path.join(notebook_directory, 'data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "combined_metadata_path = os.path.join(output_dir, 'combined_metadata.csv')\n",
    "combined_metadata.to_csv(combined_metadata_path, index=False)\n",
    "print(f\"Combined metadata {combined_metadata.shape} has been saved or updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fd368",
   "metadata": {},
   "source": [
    "## Step 7: Identify columns cluster\n",
    "\n",
    "Checking cluster of columns based on Duplicate(%) and Fill(%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e66d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_metadata_clusters import run_dash_app\n",
    "\n",
    "# Run the Dash app  \n",
    "run_dash_app(combined_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62065b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.df_filtering import filter_metadata_and_dataframes, process_dataframe\n",
    "from src.scripts.df_fuzzywuzzy import fuzzy_dataframe\n",
    "\n",
    "\n",
    "# Specify your datetime checks as a list of tuples\n",
    "datetime_checks = [\n",
    "    # ('created_t', 'created_datetime'),\n",
    "    # ('last_modified_t', 'last_modified_datetime')\n",
    "]\n",
    "\n",
    "# Specify your field frequency checks as a list of tuples\n",
    "field_checks = [\n",
    "    (['countries', 'countries_tags', 'countries_fr'], 'countries'),\n",
    "    (['ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n'], 'ingredients_palm_oil'),\n",
    "    (['nutrition_grade_fr', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g'], 'nutrition'),\n",
    "    #(['brands_tags', 'brands'], 'brands'),\n",
    "    #(['additives_n', 'additives', 'additives_tags', 'additives_fr'], 'additives'),\n",
    "    (['states', 'states_tags', 'states_fr'], 'states')\n",
    "]\n",
    "\n",
    "# Columns to check for at least one non-null value\n",
    "columns_to_check = [\n",
    "    'nutrition_grade_fr', 'energy_100g', 'fat_100g', 'saturated-fat_100g',\n",
    "    'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g',\n",
    "    'fiber_100g', 'proteins_100g', 'salt_100g', 'sodium_100g', 'vitamin-a_100g',\n",
    "    'vitamin-c_100g', 'calcium_100g', 'iron_100g', 'nutrition-score-fr_100g',\n",
    "    'nutrition-score-uk_100g'\n",
    "]\n",
    "\n",
    "# Fields to be deleted after anaylysis\n",
    "fields_to_delete = ['url', 'created_t', 'last_modified_t', 'states', 'states_tags', 'states_fr', 'countries', 'countries_fr', 'brands', 'additives_n', 'additives_tags', 'additives_fr', 'creator','ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n']\n",
    "\n",
    "# Now process your specific DataFrame\n",
    "df_name = 'fr.openfoodfacts.org.products'\n",
    "\n",
    "# Filter and process DataFrame in one step\n",
    "if df_name in dfs:\n",
    "    combined_metadata, filtered_dfs = filter_metadata_and_dataframes(combined_metadata, dfs)\n",
    "    process_dataframe(filtered_dfs[df_name], log_dir='logs', temp_dir='temp', datetime_checks=datetime_checks, field_checks=field_checks)\n",
    "    fuzzy_dataframe(temp_dir='temp', config_dir='config', checks=field_checks, threshold=90)\n",
    "\n",
    "    # Drop rows where all specified columns are null\n",
    "    filtered_dfs[df_name].dropna(subset=columns_to_check, how='all', inplace=True)\n",
    "\n",
    "    # Drop duplicates after filtering rows\n",
    "    filtered_dfs[df_name].drop_duplicates(inplace=True)\n",
    "\n",
    "    # Delete the specified columns from combined_metadata and update the related DataFrame\n",
    "    combined_metadata = combined_metadata[~combined_metadata['Column Name'].isin(fields_to_delete)]\n",
    "    filtered_dfs[df_name] = filtered_dfs[df_name][combined_metadata['Column Name']]\n",
    "\n",
    "    # Save the processed DataFrame to the dataset directory\n",
    "    dataset_path = os.path.join('dataset', f'processed_{df_name}.csv')\n",
    "    filtered_dfs[df_name].to_csv(dataset_path, index=False)\n",
    "    \n",
    "    # Save the updated metadata to the data directory\n",
    "    metadata_path = os.path.join('data', f'processed_metadata.csv')\n",
    "    combined_metadata.to_csv(metadata_path, index=False)\n",
    "    \n",
    "    print(f\"Processed DataFrame '{df_name}' and metadata have been saved.\")\n",
    "else:\n",
    "    print(f\"DataFrame '{df_name}' not found in the loaded DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ea5846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8051/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x16a0ab1b9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.scripts.plot_nutriscore import run_dash_app_nutriscore, safe_eval\n",
    "# Import necessary libraries and custom modules\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Change the current working directory to 'src'\n",
    "os.chdir(os.path.join(os.getcwd(), 'src'))\n",
    "notebook_directory =os.getcwd()\n",
    "\n",
    "# Assuming notebook_directory is already defined in your notebook\n",
    "nutriscore_directory = os.path.join(notebook_directory, 'temp', 'nutrition_combination_log.csv')\n",
    "nutriscore = pd.read_csv(nutriscore_directory)\n",
    "\n",
    "\n",
    "\n",
    "run_dash_app_nutriscore(nutriscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1bcd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nutrition_combination</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>nutrition_grade_fr</th>\n",
       "      <th>nutrition-score-fr_100g</th>\n",
       "      <th>nutrition-score-uk_100g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(b, 0.0, 0.0)</td>\n",
       "      <td>12667</td>\n",
       "      <td>b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(b, 1.0, 1.0)</td>\n",
       "      <td>11014</td>\n",
       "      <td>b</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(d, 14.0, 14.0)</td>\n",
       "      <td>10518</td>\n",
       "      <td>d</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(b, 2.0, 2.0)</td>\n",
       "      <td>10131</td>\n",
       "      <td>b</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(a, -1.0, -1.0)</td>\n",
       "      <td>8783</td>\n",
       "      <td>a</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nutrition_combination  Frequency nutrition_grade_fr  \\\n",
       "1         (b, 0.0, 0.0)      12667                  b   \n",
       "2         (b, 1.0, 1.0)      11014                  b   \n",
       "3       (d, 14.0, 14.0)      10518                  d   \n",
       "4         (b, 2.0, 2.0)      10131                  b   \n",
       "5       (a, -1.0, -1.0)       8783                  a   \n",
       "\n",
       "   nutrition-score-fr_100g  nutrition-score-uk_100g  \n",
       "1                      0.0                      0.0  \n",
       "2                      1.0                      1.0  \n",
       "3                     14.0                     14.0  \n",
       "4                      2.0                      2.0  \n",
       "5                     -1.0                     -1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutriscore.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
