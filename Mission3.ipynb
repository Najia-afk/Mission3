{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21ee40d-c122-484d-add4-55dc03e3dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Adjust display options to show all columns\n",
    "pd.set_option('display.max_columns', 5)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Relative path to the dataset directory\n",
    "dataset_directory = 'Dataset'\n",
    "\n",
    "# Get the current working directory (notebook's directory)\n",
    "notebook_directory = os.getcwd()\n",
    "\n",
    "# Construct full path to the dataset directory\n",
    "directory = os.path.join(notebook_directory, dataset_directory)\n",
    "\n",
    "# Check if the dataset directory exists\n",
    "if not os.path.exists(directory):\n",
    "    print(f\"Error: Directory '{directory}' does not exist.\")\n",
    "else:\n",
    "    # List all files in the dataset directory, ignoring .gitignore\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and f != '.gitignore']\n",
    "\n",
    "    # Dictionary to store DataFrames\n",
    "    dfs = {}\n",
    "\n",
    "    # Function to display loaded DataFrames\n",
    "    def show_loaded_dfs(df_names=None):\n",
    "        print(\"Currently loaded DataFrames:\")\n",
    "        if df_names is None:\n",
    "            for name, df in dfs.items():\n",
    "                print(f\"DataFrame for file '{name}':\")\n",
    "                print(df.head())\n",
    "                print(\"\\n\")\n",
    "        else:\n",
    "            for name in df_names:\n",
    "                if name in dfs:\n",
    "                    print(f\"DataFrame for file '{name}':\")\n",
    "                    print(dfs[name].head())\n",
    "                    print(\"\\n\")\n",
    "                else:\n",
    "                    print(f\"DataFrame '{name}' not found in the loaded DataFrames.\\n\")\n",
    "\n",
    "    # Function to preprocess DataFrames by dropping empty columns\n",
    "    def preprocess_df(df):\n",
    "        return df.dropna(axis=1, how='all')\n",
    "\n",
    "    # Function to handle and print bad lines\n",
    "    def handle_bad_line(line):\n",
    "        print(f\"Bad line encountered: {line}\")\n",
    "        return None  # Skip the bad line\n",
    "\n",
    "    # Load each file into a DataFrame, preprocess, and create dynamic variables\n",
    "    for file in files:\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        file_path = os.path.join(directory, file)\n",
    "        try:\n",
    "            # Read the file with a tab delimiter and handle bad lines using the python engine\n",
    "            df = pd.read_csv(file_path, delimiter='\\t', on_bad_lines=handle_bad_line, engine='python')\n",
    "            if df is not None:\n",
    "                df = preprocess_df(df)  # Preprocess the DataFrame\n",
    "                dfs[file_name] = df\n",
    "                globals()[file_name] = df  # Create a dynamic variable in the global namespace\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"ParserError: {e} occurred while processing file '{file}'. Skipping this file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing file '{file}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e825bf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently loaded DataFrames:\n",
      "DataFrame for file 'fr.openfoodfacts.org.products':\n",
      "            code                                                url  ... nutrition-score-fr_100g nutrition-score-uk_100g\n",
      "0  0000000003087  http://world-fr.openfoodfacts.org/produit/0000...  ...                     NaN                     NaN\n",
      "1  0000000004530  http://world-fr.openfoodfacts.org/produit/0000...  ...                    14.0                    14.0\n",
      "2  0000000004559  http://world-fr.openfoodfacts.org/produit/0000...  ...                     0.0                     0.0\n",
      "3  0000000016087  http://world-fr.openfoodfacts.org/produit/0000...  ...                    12.0                    12.0\n",
      "4  0000000016094  http://world-fr.openfoodfacts.org/produit/0000...  ...                     NaN                     NaN\n",
      "\n",
      "[5 rows x 146 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show loaded DataFrames\n",
    "show_loaded_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80310ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust display options to show all columns\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def analyze_column(col):\n",
    "    col_data = col.dropna()\n",
    "    col_dtypes = col.dtypes\n",
    "    \n",
    "    if col_data.empty:\n",
    "        col_type = 'NaN'\n",
    "        fill_percentage = 0.0\n",
    "        nan_percentage = 100.0\n",
    "        bad_null_percentage = 0.0\n",
    "    else:\n",
    "        type_counts = col_data.apply(lambda x: type(x).__name__).value_counts(normalize=True) * 100\n",
    "        if len(type_counts) == 1:\n",
    "            if type_counts.index[0] != 'NaN':\n",
    "                max_length = col_data.apply(lambda x: len(str(x))).max()\n",
    "                col_type = f\"{type_counts.index[0]}({max_length})\"\n",
    "            else:\n",
    "                col_type = type_counts.index[0]\n",
    "        else:\n",
    "            # If multiple types are present, compute errorType with percentages\n",
    "            error_type_details = ', '.join([f\"{t}: {p:.2f}%\" for t, p in type_counts.items()])\n",
    "            col_type = f\"errorType({error_type_details})\"\n",
    "        \n",
    "        fill_percentage = col_data.size / col.size * 100\n",
    "        nan_percentage = col.isna().sum() / col.size * 100\n",
    "        \n",
    "        # Check for other forms of null values\n",
    "        bad_null_count = col.isin(['', 'None', 'NULL', 'null']).sum()\n",
    "        bad_null_percentage = bad_null_count / col.size * 100\n",
    "\n",
    "    return {\n",
    "        'Column Name': col.name,\n",
    "        'Dtype': col_dtypes,\n",
    "        'Type': col_type,\n",
    "        'Fill Percentage': fill_percentage,\n",
    "        'NaN Percentage': nan_percentage,\n",
    "        'Bad Null Percentage': bad_null_percentage\n",
    "    }\n",
    "\n",
    "def analyze_dataframe(df):\n",
    "    columns_info = []\n",
    "    num_rows = len(df)\n",
    "    for col_name in df.columns:\n",
    "        col_info = analyze_column(df[col_name])\n",
    "        columns_info.append(col_info)\n",
    "    df_info = pd.DataFrame(columns_info)\n",
    "    return df_info, num_rows\n",
    "\n",
    "def create_metadata_dfs(dfs):\n",
    "    metadata_dfs = {}\n",
    "    for df_name, df in dfs.items():\n",
    "        metadata_df, num_rows = analyze_dataframe(df)\n",
    "        metadata_dfs[f'metadata_{df_name} {df.shape}'] = metadata_df\n",
    "    return metadata_dfs\n",
    "\n",
    "# Function to display metadata DataFrames\n",
    "def display_metadata_dfs(metadata_dfs):\n",
    "    for name, metadata_df in metadata_dfs.items():\n",
    "        print(f\"Metadata for {name}:\")\n",
    "        print(metadata_df)\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Analyze DataFrames and create metadata DataFrames\n",
    "metadata_dfs = create_metadata_dfs(dfs)\n",
    "\n",
    "\n",
    "\n",
    "# Save the combined metadata DataFrame to a CSV file\n",
    "combined_metadata = pd.concat(metadata_dfs.values(), keys=metadata_dfs.keys()).reset_index(level=0).rename(columns={'level_0': 'DataFrame'})\n",
    "combined_metadata.to_csv('data/combined_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f748b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the metadata DataFrames\n",
    "display_metadata_dfs(metadata_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(dfs, ignore_fields={}, mandatory_fields={}):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in the DataFrames and if no raw duplicates are found, checks for composite key duplicates.\n",
    "    \n",
    "    Parameters:\n",
    "    - dfs (dict): Dictionary of DataFrames to check.\n",
    "    - ignore_fields (dict): Dictionary where keys are DataFrame names and values are lists of column names to ignore.\n",
    "    - mandatory_fields (dict): Dictionary where keys are DataFrame names and values are lists of column names to use as composite keys.\n",
    "    \n",
    "    Returns:\n",
    "    - result (dict): Dictionary where keys are DataFrame names and values are tuples containing the number of raw duplicate rows and composite key duplicate rows.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    for df_name, df in dfs.items():\n",
    "        if df_name in ignore_fields:\n",
    "            # Drop the specified columns to ignore\n",
    "            df_to_check = df.drop(columns=ignore_fields[df_name], errors='ignore')\n",
    "        else:\n",
    "            df_to_check = df\n",
    "        \n",
    "        # Find raw duplicates\n",
    "        duplicate_rows = df_to_check.duplicated(keep=False)\n",
    "        num_raw_duplicates = duplicate_rows.sum()\n",
    "        \n",
    "        # Check for composite key duplicates if no raw duplicates are found\n",
    "        num_composite_key_duplicates = 0\n",
    "        if num_raw_duplicates == 0 and df_name in mandatory_fields:\n",
    "            composite_key_columns = mandatory_fields[df_name]\n",
    "            if set(composite_key_columns).issubset(df.columns):\n",
    "                composite_key_duplicates = df.duplicated(subset=composite_key_columns, keep=False)\n",
    "                num_composite_key_duplicates = composite_key_duplicates.sum()\n",
    "        \n",
    "        # Add to result\n",
    "        result[df_name] = (num_raw_duplicates, num_composite_key_duplicates)\n",
    "        \n",
    "        print(f\"DataFrame '{df_name}': {num_raw_duplicates} raw duplicate rows found, {num_composite_key_duplicates} composite key duplicate rows found\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "ignore_fields = {\n",
    "    'EdStatsCountry-Series': ['DESCRIPTION'],\n",
    "    'EdStatsCountry': ['Special Notes'],\n",
    "    'EdStatsFootNote': ['DESCRIPTION'],\n",
    "    'EdStatsSeries':['Short definition']\n",
    "}\n",
    "\n",
    "mandatory_fields = {\n",
    "    'EdStatsCountry-Series': ['CountryCode','SeriesCode'],\n",
    "    'EdStatsCountry': ['Country Code','Table Name'],\n",
    "    'EdStatsData': ['Country Code','Indicator Code'],\n",
    "    'EdStatsFootNote':['CountryCode','SeriesCode','Year'],\n",
    "    'EdStatsSeries':['Series Code','Indicator Name']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'dfs' is the dictionary of DataFrames already loaded\n",
    "duplicate_summary = check_duplicates(dfs, ignore_fields, mandatory_fields)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
